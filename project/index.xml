<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Projects on LIANG WU</title>
    <link>https://leon-liangwu.github.io/project/</link>
    <description>Recent content in Projects on LIANG WU</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator>
    <language>en-us</language>
    <copyright>{year} &amp;copy; Liang Wu</copyright>
    <lastBuildDate>Tue, 30 Jul 2019 00:00:00 +0000</lastBuildDate>
    
	    <atom:link href="https://leon-liangwu.github.io/project/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Mask YOLO</title>
      <link>https://leon-liangwu.github.io/project/mask_yolo/</link>
      <pubDate>Tue, 30 Jul 2019 00:00:00 +0000</pubDate>
      
      <guid>https://leon-liangwu.github.io/project/mask_yolo/</guid>
      <description>

&lt;h2 id=&#34;yolo-caffe-version-with-maskrcnn&#34;&gt;YOLO Caffe version with MaskRCNN&lt;/h2&gt;

&lt;h3 id=&#34;caffe-maskyolo&#34;&gt;Caffe-MaskYolo&lt;/h3&gt;

&lt;h4 id=&#34;what-i-add-in-this-version-of-caffe&#34;&gt;What I add in this version of caffe?&lt;/h4&gt;

&lt;ul class=&#34;task-list&#34;&gt;
&lt;li&gt;&lt;label&gt;&lt;input type=&#34;checkbox&#34; checked disabled class=&#34;task-list-item&#34;&gt; Demos for object detection, mask segmentation and keypoints recognition&lt;/label&gt;&lt;/li&gt;
&lt;li&gt;&lt;label&gt;&lt;input type=&#34;checkbox&#34; checked disabled class=&#34;task-list-item&#34;&gt; YOLO v2 (RegionLossLayer) and v3 (YoloLossLayer) are supported&lt;/label&gt;&lt;/li&gt;
&lt;li&gt;&lt;label&gt;&lt;input type=&#34;checkbox&#34; checked disabled class=&#34;task-list-item&#34;&gt; Instance Mask segmentation with Yolo&lt;/label&gt;&lt;/li&gt;
&lt;li&gt;&lt;label&gt;&lt;input type=&#34;checkbox&#34; checked disabled class=&#34;task-list-item&#34;&gt; Keypoints Recognition with yolo&lt;/label&gt;&lt;/li&gt;
&lt;li&gt;&lt;label&gt;&lt;input type=&#34;checkbox&#34; disabled class=&#34;task-list-item&#34;&gt; training data preparation and training&lt;/label&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&#34;preparation&#34;&gt;preparation&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;# clone
git clone https://github.com/leon-liangwu/MaskYolo_Caffe.git --recursive

# install requirements
cd ROOT_MaskYolo
pip install -r requirements.txt

# compile box_utils
cd lib/box_utils
python setup.py build_ext --inplace

# compile caffe
cd caffe-maskyolo
cp Makefile.config.example Makefile.config
make -j
make pycaffe
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;download-pretrained-models&#34;&gt;download pretrained models&lt;/h4&gt;

&lt;p&gt;Click &lt;a href=&#34;https://www.dropbox.com/s/z1w2z8ya28v3lah/models.tgz?dl=0&#34; title=&#34;pretrained models&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt; to download pretrained models&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cd ROOT_MaskYolo
tar zxvf /your/downlaod/model/path/models.tgz ./
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;object-detection-with-yolo&#34;&gt;Object Detection with YOLO&lt;/h3&gt;

&lt;p&gt;support to use yolo v2 or v3 to detect objects in images&lt;/p&gt;

&lt;h4 id=&#34;objection-demo&#34;&gt;objection demo&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;cd tools
python yolo_inference.py [--img_path=xxx.jpg] [--model=xxx.prototxt] [--weights=xxx.caffemodel]
# Net forward time consumed: 3.96ms
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The demo result is shown below.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;assets/detection1.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h4 id=&#34;train-for-object-detection&#34;&gt;train for object detection&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;cd ROOT_MaskYolo
sh ./scripts/convert_detection.sh  #generate lmdb for detection
cd ./models/mobilenetv2-yolo/
nohup sh yolo_train.sh &amp;gt; train.log &amp;amp;
tail -f train.log
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;instance-mask-and-keypoints&#34;&gt;Instance Mask and Keypoints&lt;/h3&gt;

&lt;h4 id=&#34;instance-mask-and-keypoints-demo&#34;&gt;instance mask and keypoints demo&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;cd tools
python mask_inference.py [--img_path=xxx.jpg] [--model=xxx.prototxt] [--weights=xxx.caffemodel] 
# Net forward time consumed: 8.67ms

python kps_inference.py [--img_path=xxx.jpg] [--model=xxx.prototxt] [--weights=xxx.caffemodel] 
# Net forward time consumed: 5.58ms
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;some resulting samples are show below.
I just feed yolo results to &lt;code&gt;roi_pooing&lt;/code&gt; or &lt;code&gt;roi_alignment&lt;/code&gt; layer.&lt;/p&gt;

&lt;!--
![](assets/mask_keypoints.png)
--&gt;

&lt;h4 id=&#34;prepare-lmdb-for-mask-regression&#34;&gt;prepare lmdb for mask regression&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;coming soon
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;reference&#34;&gt;Reference&lt;/h3&gt;

&lt;blockquote&gt;
&lt;p&gt;You Only Look Once: Unified, Real-Time Object detection &lt;a href=&#34;http://arxiv.org/abs/1506.02640&#34; target=&#34;_blank&#34;&gt;http://arxiv.org/abs/1506.02640&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;YOLO9000: Better, Faster, Stronger &lt;a href=&#34;https://arxiv.org/abs/1612.08242&#34; target=&#34;_blank&#34;&gt;https://arxiv.org/abs/1612.08242&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;YOLOv3: An Incremental Improvement &lt;a href=&#34;https://pjreddie.com/media/files/papers/YOLOv3.pdf&#34; target=&#34;_blank&#34;&gt;https://pjreddie.com/media/files/papers/YOLOv3.pdf&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks&lt;/p&gt;

&lt;p&gt;Mask R-CNN&lt;/p&gt;
&lt;/blockquote&gt;
</description>
    </item>
    
    <item>
      <title>Internal Project</title>
      <link>https://leon-liangwu.github.io/project/internal-project/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      
      <guid>https://leon-liangwu.github.io/project/internal-project/</guid>
      <description>&lt;p&gt;Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.&lt;/p&gt;

&lt;p&gt;Nullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.&lt;/p&gt;

&lt;p&gt;Cras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.&lt;/p&gt;

&lt;p&gt;Suspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.&lt;/p&gt;

&lt;p&gt;Aliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
